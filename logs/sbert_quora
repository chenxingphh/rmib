nohup: ignoring input
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
05/05/2024 02:03:03 TensorBoard activated.
2 symbols found: 0 1
vocab coverage 84164/107155 | OOV occurrences 29441/10247150 (0.2873%)
Covered by pretrained vectors 98.9283%. outside pretrained: -lsb- -rsb- -lcb- -rcb- c# redmi demonetisation brexit demonetize quorans ...
top words:
? the what is i how a to in do
filtered words:
a555lf apps/sites nlus axtria options/jobs pitbull/lab envita.com holachef non-iitians self-prep ... 10500-10000 nation/s entity-relational salary.i mensurating homeofficecarriers.com 1y0-200 kalpakjian rypple.com karvachauth
05/05/2024 02:05:15 #classes: 2; #vocab: 84166
05/05/2024 02:05:42 trainable params: 141,351,940
05/05/2024 02:05:42 trainable params (exclude embeddings): 141,351,940
05/05/2024 02:05:42 { '__index__': 0,
  '__parents__': ['default', 'data/quora'],
  '__repeat__': 1,
  'alignment': 'linear',
  'batch_size': 96,
  'beta1': 0.9,
  'beta2': 0.999,
  'blocks': 3,
  'connection': 'aug',
  'cuda': True,
  'data_dir': 'data/quora',
  'deterministic': True,
  'dropout': 0.2,
  'early_stopping': 26667,
  'embedding_dim': 300,
  'embedding_mode': 'freq',
  'enc_layers': 3,
  'encoder': 'cnn',
  'epochs': 1,
  'eval_epoch': True,
  'eval_file': 'test',
  'eval_per_samples': 12800,
  'eval_per_samples_warmup': 512000,
  'eval_per_updates': 134,
  'eval_per_updates_warmup': 5334,
  'eval_subset': None,
  'eval_warmup_samples': 3584000,
  'eval_warmup_steps': 37334,
  'fix_embeddings': True,
  'fusion': 'full',
  'grad_clipping': 5,
  'hidden_size': 200,
  'ib_type': 'None',
  'kernel_sizes': [3],
  'kl_beta': 0.0,
  'log_file': 'log.txt',
  'log_per_samples': 5120,
  'log_per_updates': 54,
  'lower_case': True,
  'lr': 2e-05,
  'lr_decay_rate': 0.95,
  'lr_decay_samples': 256000,
  'lr_decay_steps': 2667,
  'lr_warmup_samples': 0,
  'lr_warmup_steps': 0,
  'max_len': 100,
  'max_loss': 999.0,
  'max_vocab': 999999,
  'metric': 'acc',
  'min_df': 5,
  'min_len': 1,
  'min_lr': 2e-05,
  'min_samples': 5120000,
  'min_steps': 53334,
  'model': 'sbert',
  'model_path': 'bert-base-uncased',
  'multi_gpu': True,
  'name': 'benchmark_sbert_',
  'num_classes': 2,
  'num_vocab': 84166,
  'output_dir': 'models/quora',
  'padding': 0,
  'prediction': 'symmetric',
  'pretrained_embeddings': 'resources/glove.840B.300d.txt',
  'resume': None,
  'save': True,
  'save_all': False,
  'seed': 32,
  'sort_by_len': False,
  'summary_dir': 'models/quora/benchmark_sbert_',
  'summary_per_logs': 20,
  'summary_per_updates': 1080,
  'tensorboard': True,
  'tolerance_samples': 2560000,
  'watch_metrics': ['auc', 'f1', 'acc'],
  'weight_decay': 0,
  'z_beat': 0.0,
  'z_ce_loss_beat': 0.0}
05/05/2024 02:05:43 train (384348) | test (10000)
05/05/2024 02:07:28 setup complete: 0:04:25s.
05/05/2024 02:07:28 Epoch: 1
05/05/2024 02:08:11 > epoch 1 updates 54 loss: 1.1096 lr: 2.0000e-05 gnorm: 6.376205/05/2024 02:08:54 > epoch 1 updates 108 loss: 0.5652 lr: 2.0000e-05 gnorm: 2.345505/05/2024 02:09:36 > epoch 1 updates 162 loss: 0.3295 lr: 2.0000e-05 gnorm: 2.443805/05/2024 02:10:19 > epoch 1 updates 216 loss: 0.7939 lr: 2.0000e-05 gnorm: 2.424805/05/2024 02:11:02 > epoch 1 updates 270 loss: 0.3647 lr: 2.0000e-05 gnorm: 2.381705/05/2024 02:11:46 > epoch 1 updates 324 loss: 0.6849 lr: 2.0000e-05 gnorm: 2.076005/05/2024 02:12:29 > epoch 1 updates 378 loss: 0.8286 lr: 2.0000e-05 gnorm: 4.297205/05/2024 02:13:12 > epoch 1 updates 432 loss: 0.4389 lr: 2.0000e-05 gnorm: 2.985705/05/2024 02:13:58 > epoch 1 updates 486 loss: 0.9224 lr: 2.0000e-05 gnorm: 4.870305/05/2024 02:14:42 > epoch 1 updates 540 loss: 0.4967 lr: 2.0000e-05 gnorm: 4.046605/05/2024 02:15:25 > epoch 1 updates 594 loss: 0.3560 lr: 2.0000e-05 gnorm: 3.313705/05/2024 02:16:08 > epoch 1 updates 648 loss: 0.1497 lr: 2.0000e-05 gnorm: 1.715405/05/2024 02:16:51 > epoch 1 updates 702 loss: 0.1488 lr: 2.0000e-05 gnorm: 1.721805/05/2024 02:17:35 > epoch 1 updates 756 loss: 0.2103 lr: 2.0000e-05 gnorm: 2.249105/05/2024 02:18:18 > epoch 1 updates 810 loss: 0.6044 lr: 2.0000e-05 gnorm: 5.870705/05/2024 02:19:01 > epoch 1 updates 864 loss: 0.4067 lr: 2.0000e-05 gnorm: 1.739105/05/2024 02:19:44 > epoch 1 updates 918 loss: 0.8460 lr: 2.0000e-05 gnorm: 7.197405/05/2024 02:20:27 > epoch 1 updates 972 loss: 0.3205 lr: 2.0000e-05 gnorm: 3.356205/05/2024 02:21:10 > epoch 1 updates 1026 loss: 0.4342 lr: 2.0000e-05 gnorm: 4.323105/05/2024 02:21:53 > epoch 1 updates 1080 loss: 0.5681 lr: 2.0000e-05 gnorm: 5.884305/05/2024 02:22:36 > epoch 1 updates 1134 loss: 0.4253 lr: 2.0000e-05 gnorm: 1.923805/05/2024 02:23:18 > epoch 1 updates 1188 loss: 0.2611 lr: 2.0000e-05 gnorm: 3.454405/05/2024 02:24:04 > epoch 1 updates 1242 loss: 0.2917 lr: 2.0000e-05 gnorm: 3.442905/05/2024 02:24:47 > epoch 1 updates 1296 loss: 0.3388 lr: 2.0000e-05 gnorm: 1.917505/05/2024 02:25:31 > epoch 1 updates 1350 loss: 0.3640 lr: 2.0000e-05 gnorm: 4.665105/05/2024 02:26:14 > epoch 1 updates 1404 loss: 0.3308 lr: 2.0000e-05 gnorm: 2.772205/05/2024 02:26:57 > epoch 1 updates 1458 loss: 0.6420 lr: 2.0000e-05 gnorm: 7.955305/05/2024 02:27:40 > epoch 1 updates 1512 loss: 0.4910 lr: 2.0000e-05 gnorm: 2.859205/05/2024 02:28:24 > epoch 1 updates 1566 loss: 0.4223 lr: 2.0000e-05 gnorm: 5.743805/05/2024 02:29:07 > epoch 1 updates 1620 loss: 0.1774 lr: 2.0000e-05 gnorm: 2.885505/05/2024 02:29:51 > epoch 1 updates 1674 loss: 0.3869 lr: 2.0000e-05 gnorm: 3.912605/05/2024 02:30:34 > epoch 1 updates 1728 loss: 0.2487 lr: 2.0000e-05 gnorm: 3.796205/05/2024 02:31:17 > epoch 1 updates 1782 loss: 0.0664 lr: 2.0000e-05 gnorm: 1.609105/05/2024 02:32:01 > epoch 1 updates 1836 loss: 0.3623 lr: 2.0000e-05 gnorm: 3.699705/05/2024 02:32:44 > epoch 1 updates 1890 loss: 0.2538 lr: 2.0000e-05 gnorm: 3.378405/05/2024 02:33:27 > epoch 1 updates 1944 loss: 0.2891 lr: 2.0000e-05 gnorm: 2.332805/05/2024 02:34:10 > epoch 1 updates 1998 loss: 0.2460 lr: 2.0000e-05 gnorm: 3.508005/05/2024 02:34:56 > epoch 1 updates 2052 loss: 0.2013 lr: 2.0000e-05 gnorm: 3.172505/05/2024 02:35:39 > epoch 1 updates 2106 loss: 0.3536 lr: 2.0000e-05 gnorm: 3.254705/05/2024 02:36:23 > epoch 1 updates 2160 loss: 0.2096 lr: 2.0000e-05 gnorm: 2.986505/05/2024 02:37:06 > epoch 1 updates 2214 loss: 0.1912 lr: 2.0000e-05 gnorm: 3.065005/05/2024 02:37:49 > epoch 1 updates 2268 loss: 0.4349 lr: 2.0000e-05 gnorm: 3.954805/05/2024 02:38:32 > epoch 1 updates 2322 loss: 0.2027 lr: 2.0000e-05 gnorm: 2.936905/05/2024 02:39:15 > epoch 1 updates 2376 loss: 0.2147 lr: 2.0000e-05 gnorm: 3.418505/05/2024 02:39:58 > epoch 1 updates 2430 loss: 0.6442 lr: 2.0000e-05 gnorm: 5.812605/05/2024 02:40:41 > epoch 1 updates 2484 loss: 0.4466 lr: 2.0000e-05 gnorm: 6.194305/05/2024 02:41:24 > epoch 1 updates 2538 loss: 0.2908 lr: 2.0000e-05 gnorm: 3.974305/05/2024 02:42:07 > epoch 1 updates 2592 loss: 0.2415 lr: 2.0000e-05 gnorm: 3.813405/05/2024 02:42:50 > epoch 1 updates 2646 loss: 0.5228 lr: 2.0000e-05 gnorm: 7.005105/05/2024 02:43:33 > epoch 1 updates 2700 loss: 0.4187 lr: 2.0000e-05 gnorm: 3.982105/05/2024 02:44:15 > epoch 1 updates 2754 loss: 0.7503 lr: 2.0000e-05 gnorm: 8.362305/05/2024 02:45:01 > epoch 1 updates 2808 loss: 0.2583 lr: 2.0000e-05 gnorm: 3.534605/05/2024 02:45:44 > epoch 1 updates 2862 loss: 0.0947 lr: 2.0000e-05 gnorm: 1.796805/05/2024 02:46:28 > epoch 1 updates 2916 loss: 0.1039 lr: 2.0000e-05 gnorm: 1.816305/05/2024 02:47:11 > epoch 1 updates 2970 loss: 0.4199 lr: 2.0000e-05 gnorm: 4.997505/05/2024 02:47:54 > epoch 1 updates 3024 loss: 0.1980 lr: 2.0000e-05 gnorm: 3.101505/05/2024 02:48:37 > epoch 1 updates 3078 loss: 0.2149 lr: 2.0000e-05 gnorm: 2.928805/05/2024 02:49:19 > epoch 1 updates 3132 loss: 0.4079 lr: 2.0000e-05 gnorm: 4.098205/05/2024 02:50:02 > epoch 1 updates 3186 loss: 0.2483 lr: 2.0000e-05 gnorm: 1.979305/05/2024 02:50:45 > epoch 1 updates 3240 loss: 0.2941 lr: 2.0000e-05 gnorm: 2.534405/05/2024 02:51:29 > epoch 1 updates 3294 loss: 0.2581 lr: 2.0000e-05 gnorm: 3.891505/05/2024 02:52:12 > epoch 1 updates 3348 loss: 0.2458 lr: 2.0000e-05 gnorm: 3.497805/05/2024 02:52:55 > epoch 1 updates 3402 loss: 0.2940 lr: 2.0000e-05 gnorm: 2.831505/05/2024 02:53:38 > epoch 1 updates 3456 loss: 0.3428 lr: 2.0000e-05 gnorm: 4.439005/05/2024 02:54:22 > epoch 1 updates 3510 loss: 0.2028 lr: 2.0000e-05 gnorm: 3.139205/05/2024 02:55:05 > epoch 1 updates 3564 loss: 0.3383 lr: 2.0000e-05 gnorm: 3.250205/05/2024 02:55:52 > epoch 1 updates 3618 loss: 0.3732 lr: 2.0000e-05 gnorm: 3.259005/05/2024 02:56:34 > epoch 1 updates 3672 loss: 0.3008 lr: 2.0000e-05 gnorm: 2.733505/05/2024 02:57:16 > epoch 1 updates 3726 loss: 0.3540 lr: 2.0000e-05 gnorm: 2.869405/05/2024 02:57:58 > epoch 1 updates 3780 loss: 0.0666 lr: 2.0000e-05 gnorm: 1.399605/05/2024 02:58:41 > epoch 1 updates 3834 loss: 0.2167 lr: 2.0000e-05 gnorm: 3.424405/05/2024 02:59:23 > epoch 1 updates 3888 loss: 0.3332 lr: 2.0000e-05 gnorm: 4.600605/05/2024 03:00:06 > epoch 1 updates 3942 loss: 0.1369 lr: 2.0000e-05 gnorm: 2.135205/05/2024 03:00:49 > epoch 1 updates 3996 loss: 0.4118 lr: 2.0000e-05 gnorm: 5.176205/05/2024 03:00:55 
evaluating:   0%|          | 0/105 [00:00<?, ?it/s]evaluating:   1%|          | 1/105 [00:00<00:19,  5.34it/s]evaluating:   2%|▏         | 2/105 [00:00<00:19,  5.31it/s]evaluating:   3%|▎         | 3/105 [00:00<00:19,  5.30it/s]evaluating:   4%|▍         | 4/105 [00:00<00:19,  5.30it/s]evaluating:   5%|▍         | 5/105 [00:00<00:18,  5.28it/s]evaluating:   6%|▌         | 6/105 [00:01<00:18,  5.28it/s]evaluating:   7%|▋         | 7/105 [00:01<00:18,  5.29it/s]evaluating:   8%|▊         | 8/105 [00:01<00:18,  5.29it/s]evaluating:   9%|▊         | 9/105 [00:01<00:18,  5.29it/s]evaluating:  10%|▉         | 10/105 [00:01<00:17,  5.28it/s]evaluating:  10%|█         | 11/105 [00:02<00:17,  5.28it/s]evaluating:  11%|█▏        | 12/105 [00:02<00:17,  5.28it/s]evaluating:  12%|█▏        | 13/105 [00:02<00:17,  5.28it/s]evaluating:  13%|█▎        | 14/105 [00:02<00:17,  5.29it/s]evaluating:  14%|█▍        | 15/105 [00:02<00:17,  5.28it/s]evaluating:  15%|█▌        | 16/105 [00:03<00:16,  5.28it/s]evaluating:  16%|█▌        | 17/105 [00:03<00:16,  5.28it/s]evaluating:  17%|█▋        | 18/105 [00:03<00:16,  5.28it/s]evaluating:  18%|█▊        | 19/105 [00:03<00:16,  5.28it/s]evaluating:  19%|█▉        | 20/105 [00:03<00:16,  5.28it/s]evaluating:  20%|██        | 21/105 [00:04<00:17,  4.68it/s]evaluating:  21%|██        | 22/105 [00:04<00:17,  4.85it/s]evaluating:  22%|██▏       | 23/105 [00:04<00:16,  4.97it/s]evaluating:  23%|██▎       | 24/105 [00:04<00:16,  5.06it/s]evaluating:  24%|██▍       | 25/105 [00:04<00:15,  5.11it/s]evaluating:  25%|██▍       | 26/105 [00:05<00:15,  5.16it/s]evaluating:  26%|██▌       | 27/105 [00:05<00:15,  5.20it/s]evaluating:  27%|██▋       | 28/105 [00:05<00:14,  5.22it/s]evaluating:  28%|██▊       | 29/105 [00:05<00:14,  5.24it/s]evaluating:  29%|██▊       | 30/105 [00:05<00:14,  5.25it/s]evaluating:  30%|██▉       | 31/105 [00:05<00:14,  5.25it/s]evaluating:  30%|███       | 32/105 [00:06<00:13,  5.26it/s]evaluating:  31%|███▏      | 33/105 [00:06<00:13,  5.27it/s]evaluating:  32%|███▏      | 34/105 [00:06<00:13,  5.27it/s]evaluating:  33%|███▎      | 35/105 [00:06<00:13,  5.27it/s]evaluating:  34%|███▍      | 36/105 [00:06<00:13,  5.27it/s]evaluating:  35%|███▌      | 37/105 [00:07<00:12,  5.27it/s]evaluating:  36%|███▌      | 38/105 [00:07<00:12,  5.27it/s]evaluating:  37%|███▋      | 39/105 [00:07<00:12,  5.27it/s]evaluating:  38%|███▊      | 40/105 [00:07<00:12,  5.27it/s]evaluating:  39%|███▉      | 41/105 [00:07<00:12,  5.26it/s]evaluating:  40%|████      | 42/105 [00:08<00:11,  5.27it/s]evaluating:  41%|████      | 43/105 [00:08<00:11,  5.27it/s]evaluating:  42%|████▏     | 44/105 [00:08<00:11,  5.27it/s]evaluating:  43%|████▎     | 45/105 [00:08<00:11,  5.27it/s]evaluating:  44%|████▍     | 46/105 [00:08<00:11,  5.27it/s]evaluating:  45%|████▍     | 47/105 [00:08<00:11,  5.27it/s]evaluating:  46%|████▌     | 48/105 [00:09<00:10,  5.27it/s]evaluating:  47%|████▋     | 49/105 [00:09<00:10,  5.27it/s]evaluating:  48%|████▊     | 50/105 [00:09<00:10,  5.27it/s]evaluating:  49%|████▊     | 51/105 [00:09<00:10,  5.27it/s]evaluating:  50%|████▉     | 52/105 [00:09<00:10,  5.26it/s]evaluating:  50%|█████     | 53/105 [00:10<00:09,  5.23it/s]evaluating:  51%|█████▏    | 54/105 [00:10<00:09,  5.21it/s]evaluating:  52%|█████▏    | 55/105 [00:10<00:09,  5.20it/s]evaluating:  53%|█████▎    | 56/105 [00:10<00:09,  5.20it/s]evaluating:  54%|█████▍    | 57/105 [00:10<00:09,  5.20it/s]evaluating:  55%|█████▌    | 58/105 [00:11<00:09,  5.20it/s]evaluating:  56%|█████▌    | 59/105 [00:11<00:08,  5.20it/s]evaluating:  57%|█████▋    | 60/105 [00:11<00:08,  5.20it/s]evaluating:  58%|█████▊    | 61/105 [00:11<00:08,  5.21it/s]evaluating:  59%|█████▉    | 62/105 [00:11<00:08,  5.21it/s]evaluating:  60%|██████    | 63/105 [00:12<00:08,  5.21it/s]evaluating:  61%|██████    | 64/105 [00:12<00:07,  5.20it/s]evaluating:  62%|██████▏   | 65/105 [00:12<00:07,  5.21it/s]evaluating:  63%|██████▎   | 66/105 [00:12<00:07,  5.21it/s]evaluating:  64%|██████▍   | 67/105 [00:12<00:07,  5.21it/s]evaluating:  65%|██████▍   | 68/105 [00:13<00:07,  5.21it/s]evaluating:  66%|██████▌   | 69/105 [00:13<00:06,  5.21it/s]evaluating:  67%|██████▋   | 70/105 [00:13<00:06,  5.21it/s]evaluating:  68%|██████▊   | 71/105 [00:13<00:06,  5.22it/s]evaluating:  69%|██████▊   | 72/105 [00:13<00:06,  5.22it/s]evaluating:  70%|██████▉   | 73/105 [00:13<00:06,  5.22it/s]evaluating:  70%|███████   | 74/105 [00:14<00:05,  5.21it/s]evaluating:  71%|███████▏  | 75/105 [00:14<00:05,  5.22it/s]evaluating:  72%|███████▏  | 76/105 [00:14<00:05,  5.22it/s]evaluating:  73%|███████▎  | 77/105 [00:14<00:05,  5.22it/s]evaluating:  74%|███████▍  | 78/105 [00:14<00:05,  5.22it/s]evaluating:  75%|███████▌  | 79/105 [00:15<00:04,  5.22it/s]evaluating:  76%|███████▌  | 80/105 [00:15<00:04,  5.22it/s]evaluating:  77%|███████▋  | 81/105 [00:15<00:04,  5.24it/s]evaluating:  78%|███████▊  | 82/105 [00:15<00:04,  5.26it/s]evaluating:  79%|███████▉  | 83/105 [00:15<00:04,  5.28it/s]evaluating:  80%|████████  | 84/105 [00:16<00:03,  5.28it/s]evaluating:  81%|████████  | 85/105 [00:16<00:03,  5.28it/s]evaluating:  82%|████████▏ | 86/105 [00:16<00:03,  5.29it/s]evaluating:  83%|████████▎ | 87/105 [00:16<00:03,  5.29it/s]evaluating:  84%|████████▍ | 88/105 [00:16<00:03,  5.29it/s]evaluating:  85%|████████▍ | 89/105 [00:17<00:03,  5.30it/s]evaluating:  86%|████████▌ | 90/105 [00:17<00:02,  5.29it/s]evaluating:  87%|████████▋ | 91/105 [00:17<00:02,  5.29it/s]evaluating:  88%|████████▊ | 92/105 [00:17<00:02,  5.29it/s]evaluating:  89%|████████▊ | 93/105 [00:17<00:02,  5.29it/s]evaluating:  90%|████████▉ | 94/105 [00:17<00:02,  5.29it/s]evaluating:  90%|█████████ | 95/105 [00:18<00:01,  5.29it/s]evaluating:  91%|█████████▏| 96/105 [00:18<00:01,  5.29it/s]evaluating:  92%|█████████▏| 97/105 [00:18<00:01,  5.29it/s]evaluating:  93%|█████████▎| 98/105 [00:18<00:01,  5.29it/s]evaluating:  94%|█████████▍| 99/105 [00:18<00:01,  5.28it/s]evaluating:  95%|█████████▌| 100/105 [00:19<00:00,  5.29it/s]evaluating:  96%|█████████▌| 101/105 [00:19<00:00,  5.28it/s]evaluating:  97%|█████████▋| 102/105 [00:19<00:00,  5.29it/s]evaluating:  98%|█████████▊| 103/105 [00:19<00:00,  5.29it/s]evaluating:  99%|█████████▉| 104/105 [00:19<00:00,  5.29it/s]                                                             Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
05/05/2024 03:01:21 
05/05/2024 03:01:21 train loss: 0.3517 lr: 2.0000e-05 gnorm: 3.6007 clip: 11
05/05/2024 03:01:21 valid loss: 0.3347 auc: 0.9467 f1: 0.8571 acc: 0.8646 [NEW BEST]
05/05/2024 03:01:21 
05/05/2024 03:01:21 Training complete.
05/05/2024 03:01:21 best dev score 0.8646 at step 4004 (epoch 1).
05/05/2024 03:01:21 best eval stats [loss: 0.3347 auc: 0.9467 f1: 0.8571 acc: 0.8646]
05/05/2024 03:01:21 Training time: 0:58:17.
05/05/2024 03:01:23 TensorBoard activated.
05/05/2024 03:02:53 #classes: 2; #vocab: 84166
05/05/2024 03:03:15 trainable params: 141,351,940
05/05/2024 03:03:15 trainable params (exclude embeddings): 141,351,940
05/05/2024 03:03:15 { '__index__': 0,
  '__parents__': ['default', 'data/quora'],
  '__repeat__': 1,
  'alignment': 'linear',
  'batch_size': 96,
  'beta1': 0.9,
  'beta2': 0.999,
  'blocks': 3,
  'connection': 'aug',
  'cuda': True,
  'data_dir': 'data/quora',
  'deterministic': True,
  'dropout': 0.2,
  'early_stopping': 26667,
  'embedding_dim': 300,
  'embedding_mode': 'freq',
  'enc_layers': 3,
  'encoder': 'cnn',
  'epochs': 1,
  'eval_epoch': True,
  'eval_file': 'test',
  'eval_per_samples': 12800,
  'eval_per_samples_warmup': 512000,
  'eval_per_updates': 134,
  'eval_per_updates_warmup': 5334,
  'eval_subset': None,
  'eval_warmup_samples': 3584000,
  'eval_warmup_steps': 37334,
  'fix_embeddings': True,
  'fusion': 'full',
  'grad_clipping': 5,
  'hidden_size': 200,
  'ib_type': 'rmib',
  'kernel_sizes': [3],
  'kl_beta': 0.01,
  'log_file': 'log.txt',
  'log_per_samples': 5120,
  'log_per_updates': 54,
  'lower_case': True,
  'lr': 2e-05,
  'lr_decay_rate': 0.95,
  'lr_decay_samples': 256000,
  'lr_decay_steps': 2667,
  'lr_warmup_samples': 0,
  'lr_warmup_steps': 0,
  'max_len': 100,
  'max_loss': 999.0,
  'max_vocab': 999999,
  'metric': 'acc',
  'min_df': 5,
  'min_len': 1,
  'min_lr': 2e-05,
  'min_samples': 5120000,
  'min_steps': 53334,
  'model': 'sbert',
  'model_path': 'bert-base-uncased',
  'multi_gpu': True,
  'name': 'benchmark_sbert_',
  'num_classes': 2,
  'num_vocab': 84166,
  'output_dir': 'models/quora',
  'padding': 0,
  'prediction': 'symmetric',
  'pretrained_embeddings': 'resources/glove.840B.300d.txt',
  'resume': None,
  'save': True,
  'save_all': False,
  'seed': 32,
  'sort_by_len': False,
  'summary_dir': 'models/quora/benchmark_sbert_',
  'summary_per_logs': 20,
  'summary_per_updates': 1080,
  'tensorboard': True,
  'tolerance_samples': 2560000,
  'watch_metrics': ['auc', 'f1', 'acc'],
  'weight_decay': 0,
  'z_beat': 0.01,
  'z_ce_loss_beat': 0.02}
05/05/2024 03:03:16 train (384348) | test (10000)
05/05/2024 03:05:04 setup complete: 0:03:40s.
05/05/2024 03:05:04 Epoch: 1
05/05/2024 03:05:50 > epoch 1 updates 54 loss: 1.1073 lr: 2.0000e-05 gnorm: 4.442705/05/2024 03:06:34 > epoch 1 updates 108 loss: 0.5834 lr: 2.0000e-05 gnorm: 2.009105/05/2024 03:07:17 > epoch 1 updates 162 loss: 0.3596 lr: 2.0000e-05 gnorm: 2.264505/05/2024 03:08:01 > epoch 1 updates 216 loss: 0.8610 lr: 2.0000e-05 gnorm: 2.413705/05/2024 03:08:46 > epoch 1 updates 270 loss: 0.3982 lr: 2.0000e-05 gnorm: 2.149405/05/2024 03:09:30 > epoch 1 updates 324 loss: 0.7192 lr: 2.0000e-05 gnorm: 1.655505/05/2024 03:10:14 > epoch 1 updates 378 loss: 0.9253 lr: 2.0000e-05 gnorm: 3.732805/05/2024 03:10:58 > epoch 1 updates 432 loss: 0.4416 lr: 2.0000e-05 gnorm: 2.634405/05/2024 03:11:42 > epoch 1 updates 486 loss: 1.0635 lr: 2.0000e-05 gnorm: 4.572905/05/2024 03:12:26 > epoch 1 updates 540 loss: 0.5317 lr: 2.0000e-05 gnorm: 3.505405/05/2024 03:13:11 > epoch 1 updates 594 loss: 0.4045 lr: 2.0000e-05 gnorm: 3.269005/05/2024 03:13:55 > epoch 1 updates 648 loss: 0.1733 lr: 2.0000e-05 gnorm: 1.663905/05/2024 03:14:39 > epoch 1 updates 702 loss: 0.1831 lr: 2.0000e-05 gnorm: 1.687505/05/2024 03:15:23 > epoch 1 updates 756 loss: 0.2738 lr: 2.0000e-05 gnorm: 2.197705/05/2024 03:16:07 > epoch 1 updates 810 loss: 0.6543 lr: 2.0000e-05 gnorm: 4.799705/05/2024 03:16:55 > epoch 1 updates 864 loss: 0.4805 lr: 2.0000e-05 gnorm: 1.343805/05/2024 03:17:38 > epoch 1 updates 918 loss: 0.9108 lr: 2.0000e-05 gnorm: 5.923705/05/2024 03:18:22 > epoch 1 updates 972 loss: 0.3913 lr: 2.0000e-05 gnorm: 3.059605/05/2024 03:19:06 > epoch 1 updates 1026 loss: 0.4457 lr: 2.0000e-05 gnorm: 3.707805/05/2024 03:19:49 > epoch 1 updates 1080 loss: 0.5889 lr: 2.0000e-05 gnorm: 4.720905/05/2024 03:20:34 > epoch 1 updates 1134 loss: 0.5000 lr: 2.0000e-05 gnorm: 1.960905/05/2024 03:21:18 > epoch 1 updates 1188 loss: 0.3000 lr: 2.0000e-05 gnorm: 2.991805/05/2024 03:22:02 > epoch 1 updates 1242 loss: 0.3271 lr: 2.0000e-05 gnorm: 3.059305/05/2024 03:22:46 > epoch 1 updates 1296 loss: 0.3711 lr: 2.0000e-05 gnorm: 1.927105/05/2024 03:23:31 > epoch 1 updates 1350 loss: 0.4290 lr: 2.0000e-05 gnorm: 4.256505/05/2024 03:24:15 > epoch 1 updates 1404 loss: 0.4086 lr: 2.0000e-05 gnorm: 2.928405/05/2024 03:24:59 > epoch 1 updates 1458 loss: 0.6070 lr: 2.0000e-05 gnorm: 6.276905/05/2024 03:25:44 > epoch 1 updates 1512 loss: 0.4894 lr: 2.0000e-05 gnorm: 2.479505/05/2024 03:26:28 > epoch 1 updates 1566 loss: 0.4857 lr: 2.0000e-05 gnorm: 4.781305/05/2024 03:27:15 > epoch 1 updates 1620 loss: 0.2113 lr: 2.0000e-05 gnorm: 2.468605/05/2024 03:27:59 > epoch 1 updates 1674 loss: 0.4210 lr: 2.0000e-05 gnorm: 3.460505/05/2024 03:28:43 > epoch 1 updates 1728 loss: 0.2910 lr: 2.0000e-05 gnorm: 3.234505/05/2024 03:29:27 > epoch 1 updates 1782 loss: 0.0787 lr: 2.0000e-05 gnorm: 1.515505/05/2024 03:30:12 > epoch 1 updates 1836 loss: 0.4376 lr: 2.0000e-05 gnorm: 3.689805/05/2024 03:30:56 > epoch 1 updates 1890 loss: 0.3213 lr: 2.0000e-05 gnorm: 3.249105/05/2024 03:31:41 > epoch 1 updates 1944 loss: 0.3541 lr: 2.0000e-05 gnorm: 2.365905/05/2024 03:32:25 > epoch 1 updates 1998 loss: 0.2831 lr: 2.0000e-05 gnorm: 2.898505/05/2024 03:33:10 > epoch 1 updates 2052 loss: 0.2484 lr: 2.0000e-05 gnorm: 2.948905/05/2024 03:33:54 > epoch 1 updates 2106 loss: 0.3936 lr: 2.0000e-05 gnorm: 3.061705/05/2024 03:34:39 > epoch 1 updates 2160 loss: 0.2502 lr: 2.0000e-05 gnorm: 2.373805/05/2024 03:35:23 > epoch 1 updates 2214 loss: 0.2099 lr: 2.0000e-05 gnorm: 2.599105/05/2024 03:36:07 > epoch 1 updates 2268 loss: 0.4580 lr: 2.0000e-05 gnorm: 3.507905/05/2024 03:36:52 > epoch 1 updates 2322 loss: 0.2430 lr: 2.0000e-05 gnorm: 2.533505/05/2024 03:37:36 > epoch 1 updates 2376 loss: 0.2519 lr: 2.0000e-05 gnorm: 2.954505/05/2024 03:38:23 > epoch 1 updates 2430 loss: 0.6428 lr: 2.0000e-05 gnorm: 4.567605/05/2024 03:39:08 > epoch 1 updates 2484 loss: 0.4413 lr: 2.0000e-05 gnorm: 4.661305/05/2024 03:39:52 > epoch 1 updates 2538 loss: 0.3548 lr: 2.0000e-05 gnorm: 3.543505/05/2024 03:40:37 > epoch 1 updates 2592 loss: 0.2336 lr: 2.0000e-05 gnorm: 2.897805/05/2024 03:41:22 > epoch 1 updates 2646 loss: 0.5784 lr: 2.0000e-05 gnorm: 5.455005/05/2024 03:42:06 > epoch 1 updates 2700 loss: 0.4119 lr: 2.0000e-05 gnorm: 4.266305/05/2024 03:42:51 > epoch 1 updates 2754 loss: 0.8442 lr: 2.0000e-05 gnorm: 7.423605/05/2024 03:43:35 > epoch 1 updates 2808 loss: 0.3177 lr: 2.0000e-05 gnorm: 3.290305/05/2024 03:44:19 > epoch 1 updates 2862 loss: 0.1412 lr: 2.0000e-05 gnorm: 1.832805/05/2024 03:45:03 > epoch 1 updates 2916 loss: 0.1280 lr: 2.0000e-05 gnorm: 1.480505/05/2024 03:45:48 > epoch 1 updates 2970 loss: 0.4752 lr: 2.0000e-05 gnorm: 4.415005/05/2024 03:46:32 > epoch 1 updates 3024 loss: 0.2334 lr: 2.0000e-05 gnorm: 2.737505/05/2024 03:47:17 > epoch 1 updates 3078 loss: 0.2597 lr: 2.0000e-05 gnorm: 2.638805/05/2024 03:48:01 > epoch 1 updates 3132 loss: 0.4559 lr: 2.0000e-05 gnorm: 3.510205/05/2024 03:48:45 > epoch 1 updates 3186 loss: 0.2920 lr: 2.0000e-05 gnorm: 2.137405/05/2024 03:49:33 > epoch 1 updates 3240 loss: 0.2936 lr: 2.0000e-05 gnorm: 2.599105/05/2024 03:50:18 > epoch 1 updates 3294 loss: 0.2495 lr: 2.0000e-05 gnorm: 2.733505/05/2024 03:51:02 > epoch 1 updates 3348 loss: 0.3445 lr: 2.0000e-05 gnorm: 3.424805/05/2024 03:51:46 > epoch 1 updates 3402 loss: 0.2837 lr: 2.0000e-05 gnorm: 2.292705/05/2024 03:52:31 > epoch 1 updates 3456 loss: 0.3654 lr: 2.0000e-05 gnorm: 3.568905/05/2024 03:53:16 > epoch 1 updates 3510 loss: 0.2290 lr: 2.0000e-05 gnorm: 2.649105/05/2024 03:54:00 > epoch 1 updates 3564 loss: 0.3341 lr: 2.0000e-05 gnorm: 2.633005/05/2024 03:54:45 > epoch 1 updates 3618 loss: 0.3865 lr: 2.0000e-05 gnorm: 3.219805/05/2024 03:55:29 > epoch 1 updates 3672 loss: 0.3326 lr: 2.0000e-05 gnorm: 2.511805/05/2024 03:56:14 > epoch 1 updates 3726 loss: 0.3579 lr: 2.0000e-05 gnorm: 2.418605/05/2024 03:56:58 > epoch 1 updates 3780 loss: 0.0836 lr: 2.0000e-05 gnorm: 1.315605/05/2024 03:57:43 > epoch 1 updates 3834 loss: 0.2415 lr: 2.0000e-05 gnorm: 2.677205/05/2024 03:58:27 > epoch 1 updates 3888 loss: 0.3342 lr: 2.0000e-05 gnorm: 3.724005/05/2024 03:59:11 > epoch 1 updates 3942 loss: 0.1642 lr: 2.0000e-05 gnorm: 1.990805/05/2024 03:59:58 > epoch 1 updates 3996 loss: 0.4845 lr: 2.0000e-05 gnorm: 4.212205/05/2024 04:00:05 
evaluating:   0%|          | 0/105 [00:00<?, ?it/s]evaluating:   1%|          | 1/105 [00:00<00:19,  5.27it/s]evaluating:   2%|▏         | 2/105 [00:00<00:19,  5.24it/s]evaluating:   3%|▎         | 3/105 [00:00<00:19,  5.24it/s]evaluating:   4%|▍         | 4/105 [00:00<00:19,  5.24it/s]evaluating:   5%|▍         | 5/105 [00:00<00:19,  5.22it/s]evaluating:   6%|▌         | 6/105 [00:01<00:18,  5.22it/s]evaluating:   7%|▋         | 7/105 [00:01<00:18,  5.22it/s]evaluating:   8%|▊         | 8/105 [00:01<00:18,  5.22it/s]evaluating:   9%|▊         | 9/105 [00:01<00:18,  5.22it/s]evaluating:  10%|▉         | 10/105 [00:01<00:18,  5.22it/s]evaluating:  10%|█         | 11/105 [00:02<00:18,  5.22it/s]evaluating:  11%|█▏        | 12/105 [00:02<00:17,  5.21it/s]evaluating:  12%|█▏        | 13/105 [00:02<00:17,  5.21it/s]evaluating:  13%|█▎        | 14/105 [00:02<00:17,  5.21it/s]evaluating:  14%|█▍        | 15/105 [00:02<00:17,  5.21it/s]evaluating:  15%|█▌        | 16/105 [00:03<00:17,  5.21it/s]evaluating:  16%|█▌        | 17/105 [00:03<00:16,  5.22it/s]evaluating:  17%|█▋        | 18/105 [00:03<00:16,  5.21it/s]evaluating:  18%|█▊        | 19/105 [00:03<00:16,  5.21it/s]evaluating:  19%|█▉        | 20/105 [00:03<00:16,  5.21it/s]evaluating:  20%|██        | 21/105 [00:04<00:16,  5.21it/s]evaluating:  21%|██        | 22/105 [00:04<00:15,  5.21it/s]evaluating:  22%|██▏       | 23/105 [00:04<00:15,  5.22it/s]evaluating:  23%|██▎       | 24/105 [00:04<00:15,  5.21it/s]evaluating:  24%|██▍       | 25/105 [00:04<00:15,  5.21it/s]evaluating:  25%|██▍       | 26/105 [00:04<00:15,  5.21it/s]evaluating:  26%|██▌       | 27/105 [00:05<00:14,  5.21it/s]evaluating:  27%|██▋       | 28/105 [00:05<00:14,  5.21it/s]evaluating:  28%|██▊       | 29/105 [00:05<00:14,  5.21it/s]evaluating:  29%|██▊       | 30/105 [00:05<00:14,  5.21it/s]evaluating:  30%|██▉       | 31/105 [00:05<00:14,  5.20it/s]evaluating:  30%|███       | 32/105 [00:06<00:14,  5.20it/s]evaluating:  31%|███▏      | 33/105 [00:06<00:13,  5.21it/s]evaluating:  32%|███▏      | 34/105 [00:06<00:13,  5.21it/s]evaluating:  33%|███▎      | 35/105 [00:06<00:13,  5.21it/s]evaluating:  34%|███▍      | 36/105 [00:06<00:13,  5.21it/s]evaluating:  35%|███▌      | 37/105 [00:07<00:13,  5.21it/s]evaluating:  36%|███▌      | 38/105 [00:07<00:12,  5.21it/s]evaluating:  37%|███▋      | 39/105 [00:07<00:12,  5.21it/s]evaluating:  38%|███▊      | 40/105 [00:07<00:12,  5.21it/s]evaluating:  39%|███▉      | 41/105 [00:07<00:12,  5.21it/s]evaluating:  40%|████      | 42/105 [00:08<00:12,  5.20it/s]evaluating:  41%|████      | 43/105 [00:08<00:11,  5.20it/s]evaluating:  42%|████▏     | 44/105 [00:08<00:11,  5.20it/s]evaluating:  43%|████▎     | 45/105 [00:08<00:11,  5.20it/s]evaluating:  44%|████▍     | 46/105 [00:08<00:11,  5.20it/s]evaluating:  45%|████▍     | 47/105 [00:09<00:11,  5.20it/s]evaluating:  46%|████▌     | 48/105 [00:09<00:10,  5.20it/s]evaluating:  47%|████▋     | 49/105 [00:09<00:10,  5.20it/s]evaluating:  48%|████▊     | 50/105 [00:09<00:10,  5.20it/s]evaluating:  49%|████▊     | 51/105 [00:09<00:10,  5.20it/s]evaluating:  50%|████▉     | 52/105 [00:09<00:10,  5.20it/s]evaluating:  50%|█████     | 53/105 [00:10<00:09,  5.20it/s]evaluating:  51%|█████▏    | 54/105 [00:10<00:09,  5.20it/s]evaluating:  52%|█████▏    | 55/105 [00:10<00:09,  5.15it/s]evaluating:  53%|█████▎    | 56/105 [00:10<00:09,  5.14it/s]evaluating:  54%|█████▍    | 57/105 [00:10<00:09,  5.13it/s]evaluating:  55%|█████▌    | 58/105 [00:11<00:09,  5.14it/s]evaluating:  56%|█████▌    | 59/105 [00:11<00:08,  5.14it/s]evaluating:  57%|█████▋    | 60/105 [00:11<00:08,  5.14it/s]evaluating:  58%|█████▊    | 61/105 [00:11<00:08,  5.14it/s]evaluating:  59%|█████▉    | 62/105 [00:11<00:08,  5.14it/s]evaluating:  60%|██████    | 63/105 [00:12<00:08,  5.14it/s]evaluating:  61%|██████    | 64/105 [00:12<00:07,  5.15it/s]evaluating:  62%|██████▏   | 65/105 [00:12<00:08,  4.92it/s]evaluating:  63%|██████▎   | 66/105 [00:12<00:07,  4.99it/s]evaluating:  64%|██████▍   | 67/105 [00:12<00:07,  5.04it/s]evaluating:  65%|██████▍   | 68/105 [00:13<00:07,  5.07it/s]evaluating:  66%|██████▌   | 69/105 [00:13<00:07,  5.09it/s]evaluating:  67%|██████▋   | 70/105 [00:13<00:06,  5.11it/s]evaluating:  68%|██████▊   | 71/105 [00:13<00:06,  5.13it/s]evaluating:  69%|██████▊   | 72/105 [00:13<00:06,  5.14it/s]evaluating:  70%|██████▉   | 73/105 [00:14<00:06,  5.15it/s]evaluating:  70%|███████   | 74/105 [00:14<00:06,  5.15it/s]evaluating:  71%|███████▏  | 75/105 [00:14<00:05,  5.15it/s]evaluating:  72%|███████▏  | 76/105 [00:14<00:05,  5.15it/s]evaluating:  73%|███████▎  | 77/105 [00:14<00:05,  5.16it/s]evaluating:  74%|███████▍  | 78/105 [00:15<00:05,  5.16it/s]evaluating:  75%|███████▌  | 79/105 [00:15<00:05,  5.16it/s]evaluating:  76%|███████▌  | 80/105 [00:15<00:04,  5.16it/s]evaluating:  77%|███████▋  | 81/105 [00:15<00:04,  5.16it/s]evaluating:  78%|███████▊  | 82/105 [00:15<00:04,  5.18it/s]evaluating:  79%|███████▉  | 83/105 [00:16<00:04,  5.20it/s]evaluating:  80%|████████  | 84/105 [00:16<00:04,  5.21it/s]evaluating:  81%|████████  | 85/105 [00:16<00:03,  5.21it/s]evaluating:  82%|████████▏ | 86/105 [00:16<00:03,  5.22it/s]evaluating:  83%|████████▎ | 87/105 [00:16<00:03,  5.22it/s]evaluating:  84%|████████▍ | 88/105 [00:16<00:03,  5.23it/s]evaluating:  85%|████████▍ | 89/105 [00:17<00:03,  5.23it/s]evaluating:  86%|████████▌ | 90/105 [00:17<00:02,  5.20it/s]evaluating:  87%|████████▋ | 91/105 [00:17<00:02,  5.22it/s]evaluating:  88%|████████▊ | 92/105 [00:17<00:02,  5.22it/s]evaluating:  89%|████████▊ | 93/105 [00:17<00:02,  5.22it/s]evaluating:  90%|████████▉ | 94/105 [00:18<00:02,  5.22it/s]evaluating:  90%|█████████ | 95/105 [00:18<00:01,  5.22it/s]evaluating:  91%|█████████▏| 96/105 [00:18<00:01,  5.23it/s]evaluating:  92%|█████████▏| 97/105 [00:18<00:01,  5.23it/s]evaluating:  93%|█████████▎| 98/105 [00:18<00:01,  5.22it/s]evaluating:  94%|█████████▍| 99/105 [00:19<00:01,  5.22it/s]evaluating:  95%|█████████▌| 100/105 [00:19<00:00,  5.23it/s]evaluating:  96%|█████████▌| 101/105 [00:19<00:00,  5.23it/s]evaluating:  97%|█████████▋| 102/105 [00:19<00:00,  5.23it/s]evaluating:  98%|█████████▊| 103/105 [00:19<00:00,  5.23it/s]evaluating:  99%|█████████▉| 104/105 [00:20<00:00,  5.23it/s]                                                             05/05/2024 04:00:32 
05/05/2024 04:00:32 train loss: 0.3873 lr: 2.0000e-05 gnorm: 3.1294 clip: 4
05/05/2024 04:00:32 valid loss: 0.3311 auc: 0.9456 f1: 0.8704 acc: 0.8734 [NEW BEST]
05/05/2024 04:00:32 
05/05/2024 04:00:32 Training complete.
05/05/2024 04:00:32 best dev score 0.8734 at step 4004 (epoch 1).
05/05/2024 04:00:32 best eval stats [loss: 0.3311 auc: 0.9456 f1: 0.8704 acc: 0.8734]
05/05/2024 04:00:32 Training time: 0:59:08.
